{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsary Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# importing the titanic data\n",
    "titanic_data = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bringing in code from previous task for clean titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.isnull().sum() # checking for NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Age'] = titanic_data['Age'].fillna((titanic_data['Age'].mean())) # averaging out NANs in Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.isnull().sum() # inspecting remaining NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns I won't use for the ML model\n",
    "titanic_data.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'Fare'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 5 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null object\n",
      "Age         891 non-null float64\n",
      "Fare        891 non-null float64\n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# getting view of ramaining columns and their data types\n",
    "print(titanic_data.columns) \n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1']\n",
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "titanic_data['Survived'] = titanic_data['Survived'].astype(str) # converting to str so that it can be categorical\n",
    "print(titanic_data['Survived'].unique()) # testing that there are no strange outliers\n",
    "print(titanic_data['Sex'].unique()) # inspecting unique values in Sex columns\n",
    "titanic_data['Sex'].replace('male', 1, inplace = True) # change to integers for decision tree classifier\n",
    "titanic_data['Sex'].replace('female', 0, inplace = True) # change to integers for decision tree classifier\n",
    "\n",
    "# Survived will be the dependent while Pclass, Sex, Age and the fare paid will be the independent variables for purposes of the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.      1.     22.      7.25  ]\n",
      " [ 1.      0.     38.     71.2833]\n",
      " [ 3.      0.     26.      7.925 ]\n",
      " [ 1.      0.     35.     53.1   ]\n",
      " [ 3.      1.     35.      8.05  ]]\n",
      "[['0']\n",
      " ['1']\n",
      " ['1']\n",
      " ['1']\n",
      " ['0']]\n"
     ]
    }
   ],
   "source": [
    "X = titanic_data.iloc[:,[1,2,3,4]].values #  setting up the independent variables and converting to numpy arrays\n",
    "y = titanic_data.iloc[:,0].values # setting up the dependent categorical variable and converting to a numpy array\n",
    "\n",
    "# reshaping to proper shape in numpy arrays\n",
    "X = X.reshape(-1, 4)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# printing for inspection to see if correct\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting between 2 different data sets (training and test) with 75/25 relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 4)\n",
      "(223, 4)\n",
      "(668, 1)\n",
      "(223, 1)\n"
     ]
    }
   ],
   "source": [
    "# inspecting shapes of the sets. Noted that Xtest and y_test has one more row than X_val and y_val (will be adapted to conder 178 rows for both)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bagged decision tree.Testing vs base decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=None, oob_score=False,\n",
       "         random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = DecisionTreeClassifier(max_depth = 7) # base a single decision tree\n",
    "\n",
    "ensemble = BaggingClassifier(base_estimator=base, n_estimators = 10, random_state = 7) # ensemble model to test for improved accuracy\n",
    "\n",
    "base.fit(X_train, y_train.ravel()) # fitting base model\n",
    "ensemble.fit(X_train, y_train.ravel()) # fitting ensemble (bagging) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy base: 0.7982062780269058\n",
      "Accuracy ensemble: 0.8071748878923767\n"
     ]
    }
   ],
   "source": [
    "# testing accuracies between base and ensembled bagging models\n",
    "print(\"Accuracy base:\", base.score(X_test, y_test))\n",
    "print(\"Accuracy ensemble:\", ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 1.4% increase in accuracy from incorporating an ensemble model with 10 n_estimators and 7 random states with same level of depth as base model against the base decision tree classifier with 7 levels of depth (based on testing accuracies vs test data sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a random forest decision tree classifier and testing vs base decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_rf = RandomForestClassifier(n_estimators = 100, random_state = 7, max_depth = 7) # ensemble variable updated to Random Forest Classifier\n",
    "ensemble_rf.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy base: 0.7982062780269058\n",
      "Accuracy ensemble: 0.8161434977578476\n"
     ]
    }
   ],
   "source": [
    "# testing accuracies between base and random forest classification models\n",
    "print(\"Accuracy base:\", base.score(X_test, y_test))\n",
    "print(\"Accuracy ensemble:\", ensemble_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 2.2% increase in accuracy from incorporating an ensemble model (random forest with 100 n_estimators, 7 random states and the same max depth as the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an adaboost tree classifier and testing vs base decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=100, random_state=2)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_adb = AdaBoostClassifier(base, n_estimators = 100, random_state = 2)\n",
    "ensemble_adb.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy base: 0.7982062780269058\n",
      "Accuracy ensemble: 0.7623318385650224\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy base:\", base.score(X_test, y_test))\n",
    "print(\"Accuracy ensemble:\", ensemble_adb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced accuracy from base model, even with including 100 n_estimators and a random state of 2. Might be indicative that the adaboostclassifier might be overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### picking a model above and tuning between different n_estimators and max_depth levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to determine different accuracies for different n_estimators and max depth levels \n",
    "def rf_classifier(X, Y, x, y, n, d):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators = int(n), max_depth = int(d))\n",
    "    rf.fit(X, Y.ravel())\n",
    "    \n",
    "    accuracy_score = rf.score(x, y)\n",
    "    print(\"accuracy score is:\")\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### running a few functions below to see different levels of accuracy I will get when tinkering with different max depth levels and n-estimator scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7533632286995515"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 5, 1) # n-estimators of 5 and max depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7533632286995515"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 10, 2) # n-estimators of 5 and max depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7982062780269058"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 100, 3) # n-estimators of 5 and max depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7982062780269058"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 50, 4) # n-estimators of 5 and max depth of 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8026905829596412"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 25, 5) # n-estimators of 5 and max depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8026905829596412"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 100, 5) # n-estimators of 5 and max depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8071748878923767"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier(X_train, y_train, X_test, y_test, 100, 4) # n-estimators of 100 and max depth of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### based on all above models the initial random forest model with 100 n_estimators and 7 levels of depth had the best accuracy. values. however tend to change as I rerun the scripts each time and then the above functions might show higheraccuracies than the initial rf model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
